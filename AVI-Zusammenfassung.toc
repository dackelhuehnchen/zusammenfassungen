\select@language {ngerman}
\contentsline {section}{\numberline {1}Data Types}{3}
\contentsline {section}{\numberline {2}Preprocessing}{3}
\contentsline {subsection}{\numberline {2.1}Fehlerarten}{3}
\contentsline {subsubsection}{\numberline {2.1.1}behandeln fehlender Werte}{3}
\contentsline {subsection}{\numberline {2.2}Data Cleaning}{3}
\contentsline {subsubsection}{\numberline {2.2.1}Binning}{4}
\contentsline {subsubsection}{\numberline {2.2.2}Interpolation / Approximation}{4}
\contentsline {subsubsection}{\numberline {2.2.3}Regression}{4}
\contentsline {subsection}{\numberline {2.3}Normalisierung}{4}
\contentsline {subsection}{\numberline {2.4}Segmentation}{5}
\contentsline {subsubsection}{\numberline {2.4.1}k-means}{5}
\contentsline {subsection}{\numberline {2.5}Data Reduction}{5}
\contentsline {subsubsection}{\numberline {2.5.1}Sampling}{5}
\contentsline {subsection}{\numberline {2.6}Dimensionen Reduktion}{6}
\contentsline {subsubsection}{\numberline {2.6.1}Hauptprobleme}{6}
\contentsline {subsubsection}{\numberline {2.6.2}Wie werden Dimensionen Reduziert?}{6}
\contentsline {subsubsection}{\numberline {2.6.3}PCA- Hauptkomponenten Analyse}{6}
\contentsline {section}{\numberline {3}Klassifikation}{7}
\contentsline {subsection}{\numberline {3.1}Train and Test}{7}
\contentsline {subsection}{\numberline {3.2}m-fold cross validation}{7}
\contentsline {subsubsection}{\numberline {3.2.1}Klassifikationsgenauigkeit}{7}
\contentsline {subsubsection}{\numberline {3.2.2}Klassifikations-Fehler}{7}
\contentsline {subsubsection}{\numberline {3.2.3}Confusion Matrix}{7}
\contentsline {subsubsection}{\numberline {3.2.4}True Classification Error}{8}
\contentsline {subsection}{\numberline {3.3}Entscheidungsb\IeC {\"a}ume}{8}
\contentsline {subsubsection}{\numberline {3.3.1}Construction Algorithmus}{8}
\contentsline {subsubsection}{\numberline {3.3.2}typen von Splits}{8}
\contentsline {subsubsection}{\numberline {3.3.3}Information Gain}{9}
\contentsline {subsubsection}{\numberline {3.3.4}Gini-Index}{9}
\contentsline {subsubsection}{\numberline {3.3.5}Overfitting}{9}
\contentsline {subsubsection}{\numberline {3.3.6}S\IeC {\"a}uberung zum verringern von Fehlern}{9}
\contentsline {subsubsection}{\numberline {3.3.7}Minimaler Aufwand zum S\IeC {\"a}ubern der Daten}{10}
\contentsline {subsubsection}{\numberline {3.3.8}C4.5 vorteile gegen\IeC {\"u}ber ID3}{10}
\contentsline {subsubsection}{\numberline {3.3.9}Gain-Ratio}{10}
\contentsline {subsection}{\numberline {3.4}Bayesian Classification}{10}
\contentsline {subsubsection}{\numberline {3.4.1}Bayesian Networks}{10}
\contentsline {subsubsection}{\numberline {3.4.2}Vor- und Nachteile}{10}
\contentsline {subsection}{\numberline {3.5}Neuronale Netzwerke}{11}
\contentsline {subsubsection}{\numberline {3.5.1}Vor- und Nachteile}{11}
\contentsline {subsection}{\numberline {3.6}nearest-Neighbor Classification}{11}
\contentsline {subsubsection}{\numberline {3.6.1}Vor und Nachteile}{11}
\contentsline {subsection}{\numberline {3.7}Support Vector Machines}{11}
\contentsline {subsubsection}{\numberline {3.7.1}Vor und Nachteile}{12}
\contentsline {section}{\numberline {4}Clustering}{12}
\contentsline {subsection}{\numberline {4.1}Ziele}{12}
\contentsline {subsection}{\numberline {4.2}Distanzfunktionen}{12}
